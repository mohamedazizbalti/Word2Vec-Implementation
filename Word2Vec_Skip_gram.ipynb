{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec Skip-gram Implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "3b7I5CwVg8J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random"
      ],
      "metadata": {
        "id": "TaYO65lBHVbe"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        # Initialize the Word2Vec model\n",
        "        super(Word2Vec, self).__init__()\n",
        "\n",
        "        # Store the vocabulary size and embedding dimension\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Define the embedding matrix (W). This is a learnable parameter.\n",
        "        # It maps each word to an embedding of size 'embedding_dim'\n",
        "        # Shape: (embedding_dim, vocab_size)\n",
        "        self.W = nn.Parameter(torch.rand(self.embedding_dim, self.vocab_size), requires_grad=True)\n",
        "\n",
        "        # Define the output layer which transforms the embedding vector back to a vector of size vocab_size\n",
        "        # This will output the probabilities for each word in the vocabulary\n",
        "        self.output_layer = nn.Linear(self.embedding_dim, self.vocab_size)\n",
        "\n",
        "        # Softmax function to convert logits into probabilities\n",
        "        self.softmax = F.softmax\n",
        "\n",
        "    def forward(self, x, device):\n",
        "        # x is a one-hot vector of the context word, passed as input\n",
        "\n",
        "        # Move input tensor to the specified device (GPU or CPU)\n",
        "        x = torch.tensor(x, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Perform a matrix multiplication between the input vector and the transpose of the embedding matrix (W)\n",
        "        # This gives us the embedding for the context word of size (1, embedding_dim)\n",
        "        x = torch.matmul(x, self.W.t())\n",
        "\n",
        "        # Now, x is the embedding vector for the context word of size (1, embedding_dim)\n",
        "\n",
        "        # Pass the embedding through a linear layer to get a vector of size (1, vocab_size)\n",
        "        x = self.output_layer(x)\n",
        "\n",
        "        # Apply softmax to get probabilities for each word in the vocabulary\n",
        "        x = self.softmax(x, dim=-1)\n",
        "\n",
        "        # Return the final output (probabilities for each word in the vocabulary)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mPRz_HD1hEhG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the text corpus ( Text8 )"
      ],
      "metadata": {
        "id": "Hj5_XHk_nqaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import urllib.request\n",
        "\n",
        "# Download and extract Text8\n",
        "url = \"http://mattmahoney.net/dc/text8.zip\"\n",
        "file_name = \"text8.zip\"\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall()  # Extracts 'text8' to the current directory\n",
        "\n",
        "# Read the content\n",
        "with open(\"text8\", 'r') as file:\n",
        "    corpus = file.read()\n",
        "\n",
        "print(\"First 500 characters of the corpus:\")\n",
        "print(corpus[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwgZdSrgl4lE",
        "outputId": "12fab45b-3ea3-45e9-fc70-ed6fd11dd11d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 500 characters of the corpus:\n",
            " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text into words\n",
        "words = corpus.split()\n",
        "\n",
        "print(f\"Total words: {len(words)}\")\n",
        "print(f\"First 10 words: {words[:10]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3lCivDRnvsG",
        "outputId": "c56ee460-b8ec-4f4c-f2a9-c8e9c9f42c73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words: 17005207\n",
            "First 10 words: ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Define the maximum vocabulary size (top 10,000 most frequent words)\n",
        "vocab_size = 10000\n",
        "\n",
        "# Count the frequency of each word in the corpus (list 'words')\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Get the most common words in the corpus, limited to 'vocab_size' (10000 in this case)\n",
        "most_common_words = word_counts.most_common(vocab_size)\n",
        "\n",
        "# Create the vocabulary dictionary, where each word is assigned a unique index (starting from 0)\n",
        "# The most common words are assigned indices first\n",
        "vocab = {word: idx for idx, (word, _) in enumerate(most_common_words)}\n",
        "\n",
        "# Replace any word not in the vocabulary with the special token <UNK> (for \"unknown\" words)\n",
        "processed_corpus = [word if word in vocab else \"<UNK>\" for word in words]\n"
      ],
      "metadata": {
        "id": "39NYbkiun26q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[\"<UNK>\"] = 10000"
      ],
      "metadata": {
        "id": "_BLLAlk66sRA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(vocab.keys())[list(vocab.values()).index(10000)])  # Prints <UNK>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjJCKrCg6RYj",
        "outputId": "a8939905-8b96-4666-f6a5-04e5f6dda7c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<UNK>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating training pairs"
      ],
      "metadata": {
        "id": "SxJjDhPOFasy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_skipgram_pairs(words, window_size):\n",
        "    # Initialize an empty list to store the generated word pairs\n",
        "    pairs = []\n",
        "\n",
        "    # Iterate through each word in the list 'words'\n",
        "    for i, target_word in enumerate(words):\n",
        "        # Calculate the start and end indices for the context window, ensuring they are within bounds\n",
        "        start = max(i - window_size, 0)\n",
        "        end = min(i + window_size + 1, len(words))\n",
        "\n",
        "        # Iterate through the context window, avoiding the target word itself (i != j)\n",
        "        for j in range(start, end):\n",
        "            if i != j:  # Skip the target word itself\n",
        "                # Add the (target_word, context_word) pair to the pairs list\n",
        "                pairs.append((target_word, words[j]))\n",
        "\n",
        "    # Return the generated list of (target_word, context_word) pairs\n",
        "    return pairs\n",
        "\n",
        "# Define the window size for context words (1 means the context window is one word on each side)\n",
        "window_size = 1\n",
        "\n",
        "# Generate the training pairs using the processed corpus and the defined window size\n",
        "training_pairs = generate_skipgram_pairs(processed_corpus, window_size)\n",
        "\n",
        "# Print the first 5 training pairs for inspection\n",
        "print(f\"First 5 training pairs: {training_pairs[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjbvAJLLod6G",
        "outputId": "5df78bc0-2606-4611-93bc-3182eba7871f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 training pairs: [('anarchism', 'originated'), ('originated', 'anarchism'), ('originated', 'as'), ('as', 'originated'), ('as', 'a')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a loss function"
      ],
      "metadata": {
        "id": "XftBcUX9q3br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogLoss, self).__init__()\n",
        "\n",
        "    def forward(self, inputs, targets, device):\n",
        "        # Ensure inputs and targets are on the same device\n",
        "        inputs = inputs.to(device)\n",
        "        targets = torch.tensor(targets, dtype=torch.float32).to(device)  # Make sure targets are float32\n",
        "\n",
        "        # Safe logarithm computation with a small epsilon to avoid log(0)\n",
        "        epsilon = 1e-10  # Small value to prevent log(0)\n",
        "        log_vector = torch.log(inputs + epsilon)  # Apply log with epsilon to avoid log(0)\n",
        "\n",
        "        # Compute the final log loss (dot product of one-hot targets and log vector)\n",
        "        final_vector = torch.sum(targets * log_vector, dim=-1)  # Sum over vocab_size dimension\n",
        "\n",
        "        # Return the log loss value\n",
        "        return -torch.mean(final_vector)  # Negative because we want to minimize loss"
      ],
      "metadata": {
        "id": "zXr3iyi5o-A9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_embedding_model(model, training_pairs,loss_fn, opt, epochs):\n",
        "    # Iterate over the number of epochs for training\n",
        "    for epoch in range(epochs):\n",
        "        # Iterate over each (context, target) pair in the training dataset\n",
        "        for training_pair in training_pairs:\n",
        "            # Unpack the context (input) and target (output) words from the pair\n",
        "            context, target = training_pair\n",
        "\n",
        "            # Create a one-hot vector for the context word\n",
        "            X = np.zeros(shape=(len(vocab),))  # Initialize a vector of size vocab_size filled with zeros\n",
        "            X[vocab[context]] = 1  # Set the position corresponding to the context word to 1\n",
        "\n",
        "            # Create a one-hot vector for the target word\n",
        "            y = np.zeros(shape=(len(vocab),))  # Initialize a vector of size vocab_size filled with zeros\n",
        "            y[vocab[target]] = 1  # Set the position corresponding to the target word to 1\n",
        "\n",
        "            # Convert the input and target one-hot vectors to PyTorch tensors\n",
        "            X = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "            y = torch.tensor(y, dtype=torch.float32).to(device)\n",
        "\n",
        "            # Get the model's prediction by passing the one-hot context vector (X) to the model\n",
        "            output = model(X, device)\n",
        "\n",
        "            # Calculate the loss using the model's output and the target vector (y)\n",
        "            loss = loss_fn(output, y, device)\n",
        "\n",
        "            # Zero the gradients of the optimizer (clear previous gradients)\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Perform backpropagation to compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update model parameters using the optimizer\n",
        "            opt.step()\n",
        "\n",
        "        # Print the progress after each epoch\n",
        "        print(f\"Epoch {epoch + 1} training done \\n\")"
      ],
      "metadata": {
        "id": "0P-tcSqMs2gJ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up training with GPU"
      ],
      "metadata": {
        "id": "QDYY0pQaF-22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the learning rate for optimization\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Define the size of the vocabulary and the dimension of the word embeddings\n",
        "vocab_size = 10001  # Vocabulary size (including the <UNK> token)\n",
        "embedding_dim = 300  # Size of each word embedding vector\n",
        "\n",
        "# Set the device to CUDA if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize the Word2Vec model with vocab size and embedding dimension\n",
        "embedding_model = Word2Vec(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "embedding_model.to(device)\n",
        "\n",
        "# Use Adam optimizer for updating model parameters\n",
        "optimizer = torch.optim.Adam(embedding_model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the loss function (Log Loss, typically used in classification tasks)\n",
        "criterion = LogLoss()\n",
        "\n",
        "# Select a subset of training pairs (size may vary) from the whole training set\n",
        "training_size = 1000\n",
        "train_pairs = random.sample(training_pairs, training_size)\n",
        "\n",
        "# Train the embedding model for 5 epochs\n",
        "train_embedding_model(embedding_model, train_pairs, criterion, optimizer, 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAJ5sjUB8iwD",
        "outputId": "cd70833d-b24e-412c-97ba-4372036059ff"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-46-2bc9a2c05431>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x = torch.tensor(x,dtype=torch.float32).to(device)\n",
            "<ipython-input-36-6ba067f85b8e>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets, dtype=torch.float32).to(device)  # Make sure targets are float32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 training done \n",
            "\n",
            "Epoch 2 training done \n",
            "\n",
            "Epoch 3 training done \n",
            "\n",
            "Epoch 4 training done \n",
            "\n",
            "Epoch 5 training done \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_model.W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bTtFanx9fF2",
        "outputId": "32dd5c38-1bbe-402d-ca5d-74c1c59ccee4"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0146,  0.0907, -0.2002,  ...,  0.5800,  0.6810, -0.1679],\n",
            "        [ 0.0408, -0.1323,  0.2040,  ...,  0.1933,  0.5413, -0.1152],\n",
            "        [-0.0913,  0.0943,  0.0371,  ...,  0.4337,  0.6969,  0.0116],\n",
            "        ...,\n",
            "        [-0.0133,  0.4184, -0.0737,  ...,  0.3026,  0.6211,  0.2891],\n",
            "        [-0.1414, -0.0880,  0.1212,  ...,  0.9408,  0.8237,  0.1081],\n",
            "        [ 0.0022, -0.2043, -0.1563,  ...,  0.7861,  0.6502, -0.1336]],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(word, model, vocab):\n",
        "  # Check if the word is in the vocabulary\n",
        "  if word in vocab:\n",
        "    index = vocab[word]  # Retrieve the index of the word from the vocab\n",
        "  else:\n",
        "    # If the word is not found, assign the index for the <UNK> token (typically a predefined index like 10000)\n",
        "    index = 10000\n",
        "\n",
        "  # Set the index value to 1 (this seems to override the index logic, which might be unintended)\n",
        "  index = 1  # (Note: this line overwrites the previous logic, and you might want to remove or adjust this)\n",
        "\n",
        "  # Create a one-hot vector of length equal to the size of the vocabulary\n",
        "  X = np.zeros(shape=(len(vocab),))  # Initialize a vector of zeros of length vocab_size\n",
        "  X[index] = 1  # Set the position corresponding to the word's index to 1 (one-hot encoding)\n",
        "\n",
        "  # Convert the one-hot vector to a PyTorch tensor and move it to the appropriate device (GPU or CPU)\n",
        "  X = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "\n",
        "  # Multiply the one-hot vector with the model's weight matrix (W) to get the word embedding\n",
        "  embedding = torch.matmul(X, model.W.t())  # Compute the embedding by matrix multiplication\n",
        "\n",
        "  # Print the shape of the resulting embedding (for debugging purposes)\n",
        "  print(f\"dimension of embedding is : {embedding.shape}\")\n",
        "\n",
        "  # Print the computed embedding for the given word (for debugging purposes)\n",
        "  print(f\"embedding for word '{word}' is : {embedding}\")"
      ],
      "metadata": {
        "id": "DH3ZGNsfCXZI"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_embedding(\"wawa\",embedding_model,vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh8RtVkoC3S8",
        "outputId": "a7139f0d-e434-425d-f814-6722c1edbc94"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dimension of embedding is : torch.Size([300])\n",
            "embedding for word 'wawa' is : tensor([ 9.0698e-02, -1.3226e-01,  9.4261e-02,  1.1339e-01,  1.1698e-01,\n",
            "         8.1681e-02,  1.1637e-01, -1.0738e-01, -1.2629e-01,  7.6360e-02,\n",
            "         9.2827e-02,  2.3592e-02, -5.0850e-02, -4.1952e-02, -4.4760e-02,\n",
            "         1.1629e-01,  5.4650e-02,  1.2056e-01, -4.5607e-02,  1.0455e-01,\n",
            "         2.3274e-02,  1.1124e-01,  3.6648e-02, -9.7998e-02,  6.4069e-02,\n",
            "         7.9149e-02,  1.5498e-01, -1.7485e-01, -1.3088e-02, -1.2657e-01,\n",
            "        -4.7069e-03, -1.6915e-01, -4.4700e-02,  1.5338e-01,  7.9100e-02,\n",
            "         3.2009e-02, -6.2296e-02,  3.6395e-02,  9.3597e-02,  3.9470e-02,\n",
            "        -1.1973e-01,  6.0814e-02, -6.7003e-02, -1.4318e-02, -7.3098e-02,\n",
            "         7.0064e-02, -7.7794e-02,  5.6612e-02,  2.9793e-02,  1.4543e-01,\n",
            "        -2.2176e-02,  2.0141e-02, -4.4536e-02,  1.1531e-01,  7.7149e-02,\n",
            "         1.4333e-01, -5.6273e-02, -2.9748e-02, -3.0252e-02,  1.1383e-01,\n",
            "        -5.7922e-02, -1.1934e-01,  2.1869e-02,  3.8418e-02, -1.3384e-01,\n",
            "        -1.3272e-01, -9.4005e-02,  1.1373e-01,  1.4175e-02, -1.0541e-01,\n",
            "         2.7329e-02, -3.1688e-02,  6.3387e-02,  6.2990e-02, -4.3954e-02,\n",
            "         9.7542e-02,  7.8255e-03,  7.3636e-02, -7.0492e-02, -1.2727e-01,\n",
            "         4.0601e-02, -1.2589e-01,  5.4602e-02,  5.1056e-02, -1.4295e-01,\n",
            "        -4.3277e-02, -5.9412e-02, -1.1641e-01,  3.3607e-02,  1.0863e-01,\n",
            "        -9.1525e-02, -2.4534e-03, -7.9944e-02,  1.1383e-01,  7.7277e-02,\n",
            "        -1.3220e-01, -1.3876e-01, -5.4008e-02, -1.0646e-01, -3.9381e-02,\n",
            "         1.2578e-01, -2.0371e-02, -1.3710e-01, -6.6561e-02, -1.2111e-01,\n",
            "        -2.0349e-02, -2.2512e-02, -8.6206e-02,  9.2641e-03,  6.4741e-02,\n",
            "         2.9262e-02,  5.4606e-02, -6.7241e-02,  1.7070e-01,  6.5487e-02,\n",
            "         3.7669e-02, -1.6226e-01,  1.0820e-01,  1.3787e-01, -1.2914e-01,\n",
            "        -1.1503e-01,  1.9385e-02,  7.7684e-02, -6.0380e-02, -1.2825e-01,\n",
            "        -3.3599e-02, -4.6762e-02, -1.1433e-01, -1.5093e-04,  3.0553e-02,\n",
            "        -1.6907e-01, -2.2644e-01,  1.2825e-01, -8.2373e-02, -1.0279e-01,\n",
            "         1.5217e-01, -7.1894e-02, -7.7983e-02, -9.4348e-02, -1.1039e-01,\n",
            "        -1.2291e-01, -6.0970e-02,  2.5791e-02, -9.8609e-02, -9.3240e-02,\n",
            "        -1.0104e-01, -8.8376e-02, -8.9687e-02, -1.4111e-01,  1.2607e-01,\n",
            "        -9.9629e-02, -1.0969e-01, -1.0579e-01, -1.2471e-01, -6.7334e-02,\n",
            "         6.3889e-02, -6.1319e-02, -8.4939e-02, -8.4842e-02, -4.0869e-02,\n",
            "         9.5763e-02,  6.0395e-02, -1.3218e-01, -1.1231e-01,  8.0226e-02,\n",
            "        -9.8816e-02,  6.6124e-02, -4.5894e-02,  1.4867e-01,  1.0015e-01,\n",
            "        -9.6653e-02,  4.0027e-02, -9.9832e-02, -6.9715e-02, -1.2212e-01,\n",
            "        -8.8119e-02, -9.2238e-02, -8.7578e-02,  1.3495e-02, -1.4715e-01,\n",
            "        -6.9292e-02,  5.9373e-02, -7.1501e-02,  1.2750e-01, -8.3991e-02,\n",
            "        -4.1206e-02, -1.3229e-01, -1.4146e-01,  1.4371e-01, -9.6606e-02,\n",
            "         9.3752e-02, -1.1081e-01,  3.7780e-02, -8.5189e-02,  9.6395e-02,\n",
            "         3.9231e-02,  1.4415e-01,  2.9722e-02, -1.3105e-01,  7.6243e-02,\n",
            "        -7.6676e-03, -2.6793e-02,  6.3898e-02, -1.0874e-01, -4.4337e-02,\n",
            "        -8.5891e-02, -2.4061e-02, -4.8208e-02, -6.3193e-02, -9.3014e-02,\n",
            "        -1.1493e-01, -1.0044e-01, -1.0154e-01, -1.9346e-02,  1.3554e-01,\n",
            "        -7.1656e-02,  1.2078e-02, -8.7925e-02, -5.4379e-02, -9.4172e-02,\n",
            "        -8.1989e-02, -6.8507e-02, -1.3427e-01, -5.7113e-02, -5.7276e-02,\n",
            "        -4.6352e-02,  1.0320e-01,  1.1098e-01,  6.6903e-02,  6.1695e-02,\n",
            "        -4.2342e-02, -5.0487e-02, -9.6804e-02, -3.1434e-02,  1.9098e-01,\n",
            "        -3.0916e-02, -4.8306e-02, -1.2870e-01, -5.9833e-02, -1.5908e-01,\n",
            "        -1.5644e-01, -2.3875e-02, -9.6277e-03,  1.0331e-01,  6.8748e-02,\n",
            "         4.4015e-03, -9.2359e-02, -6.8038e-02, -1.5470e-01, -1.0901e-01,\n",
            "        -4.5579e-03, -2.1054e-02,  6.3335e-02,  2.2283e-02,  1.4831e-02,\n",
            "        -1.2059e-01, -1.0040e-01, -1.8652e-02, -3.4201e-02,  1.1686e-01,\n",
            "         1.1713e-01, -8.0294e-02,  4.2511e-02, -1.2821e-01, -7.3377e-02,\n",
            "        -2.3690e-02,  9.5767e-02, -6.8553e-02, -3.8878e-02, -8.0185e-02,\n",
            "        -3.9646e-02, -2.7821e-02,  1.4217e-02, -8.9173e-02,  8.7218e-02,\n",
            "         1.1733e-01,  5.5678e-02,  1.0690e-02, -1.0348e-02, -1.0187e-01,\n",
            "         1.7650e-01, -1.2113e-01,  4.6315e-02, -6.5462e-02, -4.2827e-02,\n",
            "         2.7620e-02,  9.7319e-03,  1.0685e-01,  9.9261e-02, -9.6047e-03,\n",
            "         3.7920e-02, -6.2196e-02,  1.1056e-01,  2.8262e-02, -2.2110e-01,\n",
            "         1.2490e-01,  1.1689e-01,  4.1841e-01, -8.7972e-02, -2.0427e-01],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kOz9g9WoGvmt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}